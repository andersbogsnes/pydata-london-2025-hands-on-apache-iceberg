{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:19:36.071666Z",
     "start_time": "2025-05-05T19:19:35.247612Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyiceberg.catalog.rest import RestCatalog\n",
    "import polars as pl\n",
    "from IPython.display import JSON, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Hands-on with Apache Iceberg\n",
    "\n",
    "Welcome to this hands-on demo of working with Apache Iceberg. In this demo, our motivating task is to answer the question:\n",
    "\n",
    "**\"Is my house worth its weight in gold?\"**\n",
    "\n",
    "We will go through some fundamentals of working with Iceberg, and then work our way towards answering this important question. There will be some exercises along the way, so you can get some practice. That also means, you need to remember to execute every cell!\n",
    "\n",
    "The data we will be working with today is the **Price Paid Dataset** (aka PPD) which is published on `gov.uk`. The description of the website states\n",
    "\n",
    "> Our Price Paid Data includes information on all property sales in England and Wales that are sold for value and are lodged with us for registration.\n",
    "\n",
    "As per the license terms, we include the proper attribution and a link to OGL [here](https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/)\n",
    "> Contains HM Land Registry data © Crown copyright and database right 2021. This data is licensed under the Open Government Licence v3.0.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Before we get started, make sure you've run the following if you want to follow along:\n",
    "- `docker compose up -d` \n",
    "- `iceberg download housing` \n",
    "- `iceberg bootstrap`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30e30844-02f7-499b-9f19-2a47edfdf9e4",
   "metadata": {},
   "source": [
    "# Intro to Apache Iceberg\n",
    "\n",
    "Apache Iceberg is a specification - it merely defines how the data and metadata should be stored on disk. This is the power of standards specifications, as anyone can read and write Iceberg if they follow the specification.\n",
    "\n",
    "The specification is [fairly long and detailed](https://iceberg.apache.org/spec/), but we will walk through the core concepts to give you an understanding of what Iceberg is doing under the hood.\n",
    "\n",
    "## Iceberg Architecture\n",
    "\n",
    "This is Iceberg - after this demo, you should walk away with a pretty good understanding of what this drawing is showing\n",
    "\n",
    "![Iceberg Architecture](images/iceberg_architecture.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## The Catalog\n",
    " \n",
    "\n",
    "We can ask it for a given table name, and it will tell us where to find it. It also tracks an \"optimistic\" lock on writing to a table to preserve transactionality\n",
    "\n",
    "![Architecture Catalog](images/iceberg_architecture_catalogue.svg)\n",
    "\n",
    "The choice of catalog is the first choice to make when adopting Iceberg. \n",
    "\n",
    "Sometimes it's made for you e.g `AWS Glue` for Athena-based workloads, `Polaris/Open Catalog` for Snowflake based teams or `Unity Catalog` for the Databricks fans. \n",
    "\n",
    "Sometimes you're looking for specific features such as `Nessie`'s focus on branching workflows, or the compatibility of `Hive Meta Store`. \n",
    "\n",
    "Today we will be using `Lakekeeper`, a Rust implementation that is a single binary. Note that while Lakekeeper has full support for AuthN/AuthZ, for the sake of the demo, no protection is being used - don't do this at home kids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T19:15:30.914212Z",
     "start_time": "2025-05-04T19:15:30.871978Z"
    }
   },
   "outputs": [],
   "source": [
    "# The warehouse was created during the bootstrapping process, it's specific to Lakekeeper\n",
    "catalog = RestCatalog(\n",
    "    \"lakekeeper\", uri=\"http://lakekeeper:8181/catalog\", warehouse=\"lakehouse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a5c20-0746-4b2a-9bb5-e24ce5649a3d",
   "metadata": {},
   "source": [
    "In Iceberg, a catalog is responsible for any number of sets of tables, where a set of tables is grouped into a namespace. The catalogue can specify additional layers, such as the `warehouse` abstraction offered by Lakekeeper, though this is not required by the Iceberg spec.\n",
    "\n",
    "Since we're dealing with house prices, let's go with `housing` - you could imagine we would want to add other interesting housing-related datasets at a later point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.create_namespace_if_not_exists(\"housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Schema\n",
    "\n",
    "Next we need a schema. In PyIceberg, we can define the schema using Pyiceberg types, though many query engines also support creating Iceberg tables via SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.schema import Schema, NestedField, StringType, IntegerType, DateType\n",
    "\n",
    "housing_prices_schema = Schema(\n",
    "    NestedField(\n",
    "        1,\n",
    "        \"transaction_id\",\n",
    "        StringType(),\n",
    "        required=True,\n",
    "        doc=\"A reference number which is generated automatically recording each published sale. The number is unique and will change each time a sale is recorded.\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        2,\n",
    "        \"price\",\n",
    "        IntegerType(),\n",
    "        required=True,\n",
    "        doc=\"Sale price stated on the transfer deed.\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        3,\n",
    "        \"date_of_transfer\",\n",
    "        DateType(),\n",
    "        required=True,\n",
    "        doc=\"Date when the sale was completed, as stated on the transfer deed.\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        4,\n",
    "        \"postcode\",\n",
    "        StringType(),\n",
    "        required=True,\n",
    "        doc=\"This is the postcode used at the time of the original transaction. Note that postcodes can be reallocated and these changes are not reflected in the Price Paid Dataset.\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        5,\n",
    "        \"property_type\",\n",
    "        StringType(),\n",
    "        required=True,\n",
    "        doc=\"D = Detached, S = Semi-Detached, T = Terraced, F = Flats/Maisonettes, O = Other\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        6,\n",
    "        \"new_property\",\n",
    "        StringType(),\n",
    "        required=True,\n",
    "        doc=\"Indicates the age of the property and applies to all price paid transactions, residential and non-residential. Y = a newly built property, N = an established residential building\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        7,\n",
    "        \"duration\",\n",
    "        StringType(),\n",
    "        required=True,\n",
    "        doc=\"Relates to the tenure: F = Freehold, L= Leasehold etc. Note that HM Land Registry does not record leases of 7 years or less in the Price Paid Dataset.\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        8,\n",
    "        \"paon\",\n",
    "        StringType(),\n",
    "        doc=\"Primary Addressable Object Name. Typically the house number or name\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        9,\n",
    "        \"saon\",\n",
    "        StringType(),\n",
    "        doc=\"Secondary Addressable Object Name. Where a property has been divided into separate units (for example, flats), the PAON (above) will identify the building and a SAON will be specified that identifies the separate unit/flat.\",\n",
    "    ),\n",
    "    NestedField(10, \"street\", StringType()),\n",
    "    NestedField(11, \"locality\", StringType()),\n",
    "    NestedField(12, \"town\", StringType()),\n",
    "    NestedField(13, \"district\", StringType()),\n",
    "    NestedField(14, \"county\", StringType()),\n",
    "    NestedField(\n",
    "        15,\n",
    "        \"ppd_category_type\",\n",
    "        StringType(),\n",
    "        doc=\"Indicates the type of Price Paid transaction. A = Standard Price Paid entry, includes single residential property sold for value. B = Additional Price Paid entry including transfers under a power of sale/repossessions, buy-to-lets (where they can be identified by a Mortgage), transfers to non-private individuals and sales where the property type is classed as ‘Other’.\",\n",
    "    ),\n",
    "    NestedField(\n",
    "        16,\n",
    "        \"record_status\",\n",
    "        StringType(),\n",
    "        doc=\"Indicates additions, changes and deletions to the records. A = Addition C = Change D = Delete\",\n",
    "    ),\n",
    "    identifier_field_ids=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## The Table\n",
    "\n",
    "With our schema in place, we're now ready to create the table. Here we are directly specifying the location where the table will be stored, though depending on the catalog, it can automatically assign a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_t = catalog.create_table_if_not_exists(\n",
    "    \"housing.staging_prices\",\n",
    "    schema=housing_prices_schema,\n",
    "    location=\"s3://warehouse/housing/staging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## The Data\n",
    "Pyiceberg relies on Apache Arrow as its Data Interchange Format, so we need to read in our CSV files and convert them to Arrow. \n",
    "\n",
    "We need to do some light processing of the data, like setting the headers and casting the dtypes properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_house_prices(csv_filename: str) -> pl.DataFrame:\n",
    "    \"\"\"Read a CSV file and return a polars Dataframe\"\"\"\n",
    "    # Columns sourced from data dictionary\n",
    "    house_prices_columns = [\n",
    "        \"transaction_id\",\n",
    "        \"price\",\n",
    "        \"date_of_transfer\",\n",
    "        \"postcode\",\n",
    "        \"property_type\",\n",
    "        \"new_property\",\n",
    "        \"duration\",\n",
    "        \"paon\",\n",
    "        \"saon\",\n",
    "        \"street\",\n",
    "        \"locality\",\n",
    "        \"town\",\n",
    "        \"district\",\n",
    "        \"county\",\n",
    "        \"ppd_category_type\",\n",
    "        \"record_status\",\n",
    "    ]\n",
    "\n",
    "    df = (\n",
    "        pl.scan_csv(\n",
    "            csv_filename,\n",
    "            has_header=False,\n",
    "            new_columns=house_prices_columns,\n",
    "        )\n",
    "        .with_columns(pl.col(\"date_of_transfer\").str.to_date(\"%Y-%m-%d %H:%M\"))\n",
    "        .collect()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "house_prices_2024 = read_house_prices(\"data/house_prices/pp-2024.csv\")\n",
    "house_prices_2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Writing the Data\n",
    "Now that we have the data loaded, we're ready to write it out to our Iceberg table. We have 3 different strategies available to us:\n",
    "\n",
    "- append\n",
    "- overwrite\n",
    "- upsert\n",
    "\n",
    "`append` will write the data to the end of the table. \n",
    "\n",
    "`overwrite` will delete the existing data before appending - though there is an optional `overwrite_filter` to delete a subset matching that filter before appending.\n",
    "\n",
    "`upsert` is a recent addition to Pyiceberg (0.9.0), where given a key column, it will compare the keys to existing rows to decide if a given row should be updated or inserted.\n",
    "\n",
    "W let's stick to `append`.\n",
    "\n",
    "```{note} Note on schemas\n",
    "PyIceberg is strict on the schema - by default, Polars is a bit looser, so we need to `cast` the exported polars arrow table into the same schema as we've defined - otherwise our write will be rejected.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to arrow and cast it\n",
    "house_prices_arrow = (\n",
    "    house_prices_2024.to_arrow().cast(  # Export to an Arrow table\n",
    "        housing_prices_schema.as_arrow()\n",
    "    )  # Cast into the Iceberg schema\n",
    ")\n",
    "# Append data to the table\n",
    "house_prices_t.append(house_prices_arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Metadata - the secret of Iceberg\n",
    "\n",
    "Now that we've created a schema for our houseprices, let's take a look at the metadata that we've created. In Iceberg, all the metadata is stored in a combination of JSON and Avro, and all the metadata is stored in the S3 buckets directly, which is what makes it accessible from the various query engines.\n",
    "\n",
    "Let's have a look at the different files we've created out of the box. First, we need something that can talk to S3 - in this case our Minio S3 - enter fsspec and s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    endpoint_url=\"http://minio:9000\", key=\"minio\", secret=\"minio1234\"\n",
    ")\n",
    "fs.ls(\"/warehouse/housing/staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Now that we have something that can read our S3 bucket in Minio, we need to know where our Iceberg Catalogue put our most recent table update. PyIceberg stores that information in the `metadata_location` of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_t.metadata_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "That's a gzipped json file, an implementation choice from our Iceberg Rest Catalog, so we need to do some extra work to read our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsspec import AbstractFileSystem\n",
    "from pyiceberg.table import Table\n",
    "from typing import Any\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "\n",
    "def get_iceberg_metadata(fs: AbstractFileSystem, table: Table) -> dict[str, Any]:\n",
    "    \"\"\"Unzips the gzipped json and reads it into a dictionary\"\"\"\n",
    "    with fs.open(table.metadata_location) as f, gzip.open(f) as g_f:\n",
    "        return json.load(g_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We've asked the Iceberg Catalogue to give us the location of the snapshot file for our `housing.staging_prices` table. \n",
    "\n",
    "Let's have a look at the contents of the current metadata.json to get a better understanding of how Iceberg does what it does.\n",
    "\n",
    "![Iceberg metadata](images/iceberg_architecture_metadata.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_metadata(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The next layer of the onion is the `manifest list` - we can find that by looking in the `snapshots` array for the current snapshot. The manifest list is stored as Avro for faster scanning so we need to convert it to a dictionary for our simple human brains to understand.\n",
    "\n",
    "![Iceberg manifest list](images/iceberg_architecture_manifest_list.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iceberg_manifest_list(fs: AbstractFileSystem, table: Table) -> dict[str, Any]:\n",
    "    \"\"\"Fetch the manifest list for the current snapshot and convert to a list of dicts\"\"\"\n",
    "    manifest_list = table.current_snapshot().manifest_list\n",
    "    with fs.open(manifest_list) as f:\n",
    "        return pl.read_avro(f).to_dicts()\n",
    "\n",
    "\n",
    "JSON(get_iceberg_manifest_list(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We can see that the manifest list unsurprisingly contains a list of manifests, alongside some metadata for that particular manifest. We've only written once to our Iceberg table, so we have only one manifest currently. Let's dig one level deeper and open that manifest.\n",
    "\n",
    "![Iceberg Manifest](images/iceberg_architecture_manifest.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iceberg_manifest(\n",
    "    fs: AbstractFileSystem, table: Table\n",
    ") -> list[list[dict[str, Any]]]:\n",
    "    \"\"\"Get the manifests from the manifest list.\"\"\"\n",
    "    manifest_list = get_iceberg_manifest_list(fs, table)\n",
    "    manifest_lists = []\n",
    "    for manifest_meta in manifest_list:\n",
    "        with fs.open(manifest_meta[\"manifest_path\"]) as m_f:\n",
    "            manifest = pl.read_avro(m_f).to_dicts()\n",
    "            manifest_lists.extend(manifest)\n",
    "    return manifest_lists\n",
    "\n",
    "\n",
    "JSON(get_iceberg_manifest(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "At the lowest level of metadata, we can see a reference to the actual data files that make up the physical data stored on disk.\n",
    "\n",
    "![Iceberg Data Files](images/iceberg_architecture_data.svg)\n",
    "\n",
    "```{note} Iceberg vs Hive\n",
    "Note that Iceberg keeps track of the physical files of the table, unlike something like Hive, which uses a folder as a logical container for a table.\n",
    "```\n",
    "\n",
    "We can see that the Parquet file is pretty much as we expected, and we can read it directly as any other Parquet files - Iceberg doesn't specify anything about the physical data - it just stores metadata about the files to enable all the features of Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iceberg_data_file(\n",
    "    fs: AbstractFileSystem, table: Table, index=0\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Read the data file from the `index` position in the data_file\"\"\"\n",
    "    manifest = get_iceberg_manifest(fs, table)\n",
    "    with fs.open(manifest[index][\"data_file\"][\"file_path\"]) as p_f:\n",
    "        return pl.read_parquet(p_f)\n",
    "\n",
    "\n",
    "get_iceberg_data_file(fs, house_prices_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c6af1-08ff-4b41-8d90-6f888cf4b6c7",
   "metadata": {},
   "source": [
    "In total, on disk, the current data comes to around 15 MB, which is pretty small, so we end up only having one data file in our manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d5af6-232b-4511-aeec-e2d38bbcb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iceberg_manifest(fs, house_prices_t)[0][\"data_file\"][\"file_size_in_bytes\"] / 1024 / 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b5c90cb-52f3-49dc-8810-b1671b6cd4ff",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "To see what changes in our metadata when we add more data, let's add one more month of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_2023 = read_house_prices(\"data/house_prices/pp-2023.csv\")\n",
    "house_prices_t.append(\n",
    "    house_prices_2023.to_arrow().cast(housing_prices_schema.as_arrow())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Now that we have some more data, and based on what we've learnt - what do you see that has changed in the metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_metadata(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_manifest_list(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_manifest(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8788b-0d14-41d9-a3ac-185fa198ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"/warehouse/housing/staging/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCnT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Concluding on Metadata\n",
    "\n",
    "All the metadata we've looked at here is stored in object storage. It's this metadata which powers all of Iceberg - if you can understand how this metadata is put together, you understand the inner workings of Iceberg."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
