{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog.rest import RestCatalog\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "import sqlalchemy as sa\n",
    "\n",
    "# The functions we defined in the previous notebook are defined in utils.py\n",
    "from utils import get_iceberg_metadata, read_house_prices\n",
    "from s3fs import S3FileSystem\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Updating Metadata\n",
    "\n",
    "We've added data to our tables and inspected how Iceberg keeps track of the data in the metadata files\n",
    "\n",
    "Usually when working with data in real life, we make decisions that we regret in six months time. \n",
    "\n",
    "Now that we've added some data, we've found out that we've made a mistake - we should have added a `_loaded_at` column to our data, so that we can differentiate downstream between the source timestamp and our loaded time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a reference to our catalog and table again\n",
    "catalog = RestCatalog(\n",
    "    \"lakekeeper\", uri=\"http://lakekeeper:8181/catalog\", warehouse=\"lakehouse\"\n",
    ")\n",
    "house_prices_t = catalog.load_table(\"housing.staging_prices\")\n",
    "fs = S3FileSystem(endpoint_url=\"http://minio:9000\", key=\"minio\", secret=\"minio1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = dt.datetime.now(tz=dt.UTC)\n",
    "house_prices_2022 = read_house_prices(\"data/house_prices/pp-2022.csv\").with_columns(\n",
    "    pl.lit(timestamp).alias(\"_loaded_at\")\n",
    ")\n",
    "house_prices_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    house_prices_t.upsert(house_prices_2022.to_arrow())\n",
    "except ValueError as e:\n",
    "    # Print out the error message instead of crashing\n",
    "    print(e.args[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Pyiceberg is preventing us from doing something we shouldn't - Iceberg has a fixed schema, so we can't just add arbitrary columns to it. We need to update the schema to accomodate our new column.\n",
    "\n",
    "```{note}\n",
    "Pyiceberg gives us the ability to do this within a transaction to live up to Iceberg's ACID guarantees.\n",
    "```\n",
    "The new schema is added to the Iceberg metadata in the `schemas` array. Note that each of our snapshots reference the schema at the time the data was written. That way Iceberg can keep track of the schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a830ebb-4f1b-43a2-bd0b-c4ba83600e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.types import TimestamptzType\n",
    "\n",
    "with house_prices_t.update_schema() as schema:\n",
    "    schema.add_column(\n",
    "        \"_loaded_at\", TimestamptzType(), doc=\"The date this row was loaded\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966df564-d75f-4920-aa7c-e1caf20be143",
   "metadata": {},
   "source": [
    "Looking at our metadata again - what's changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d7229-ae1c-44e4-8536-1ebf1870fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_metadata(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "Now we have our `_loaded_at` column as part of the table schema, Iceberg is happy for us to add our data with the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_t.append(\n",
    "    house_prices_2022.to_arrow().cast(house_prices_t.schema().as_arrow())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d691fa-4a94-474a-830d-f8c779e40d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_metadata(fs, house_prices_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4051e2e-f873-4746-8a3e-584011c3c411",
   "metadata": {},
   "source": [
    "What about the data we already added? How would we modify that data? Here we start running into some limitations of a foundational library like `pyiceberg` - we can do it (bonus homework - how would you do it in pyiceberg natively?), but wouldn't it be much easier to write an `UPDATE` in SQL and not have to worry about the details?\n",
    "\n",
    "This is the power of Iceberg - we have the ability to switch query engines to suit our usecase - in this case, I want to use Trino to update the data back in time.\n",
    "\n",
    "Let's verify how many nulls we have - the data we just added should have `_loaded_at` filled in, but the rest should be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(\"trino://trino:@trino:8080/lakekeeper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_sql = \"\"\"\n",
    "SELECT\n",
    "    COUNT(*) as total_rows,\n",
    "    COUNT_IF(_loaded_at IS NULL) as null_count\n",
    "FROM housing.staging_prices\n",
    "    \"\"\"\n",
    "pl.read_database(null_count_sql, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb8623-bcbf-4824-9c3d-d27036e19b20",
   "metadata": {},
   "source": [
    "A simple UPDATE in SQL saves us many lines of Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    sql = f\"UPDATE housing.staging_prices SET _loaded_at = from_iso8601_timestamp('{timestamp.isoformat()}') WHERE _loaded_at is null\"\n",
    "    result = conn.execute(sa.text(sql))\n",
    "    print(f\"Updated {result.scalar_one():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_database(null_count_sql, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "We should now have a new snapshot - let's have a peek.\n",
    "\n",
    "```{warning} Keep metadata in sync\n",
    "Pyiceberg doesn't yet know about our Trino update - we need to refresh the metadata to get the latest metadata from the catalog\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade27916-3536-4ab8-a24c-1ee72bad7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_t.refresh();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_metadata(fs, house_prices_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "## Deletes\n",
    "\n",
    "We have a new operation `overwrite` - Parquet is immutable, so we have to physically write out a new file and delete the old one. That is expensive, so Iceberg uses delete files to avoid having to up-front do the work of actually deleting data.\n",
    "\n",
    "```{note} Aside\n",
    "Technically, Parquet **row groups** are immutable, but it's much faster to treat the Parquet file as immutable, rather than rewriting row groups\n",
    "```\n",
    "\n",
    "The Iceberg V2 spec defines positional-deletes and equality-deletes. These are both represented by a `delete` file, which is just a parquet file which specifies rows to mark as deleted, either by a filter like `transaction_id = '{045A1898-4ABF-9A24-E063-4804A8C048EA}'` or by position, like this:\n",
    "```{code} parquet\n",
    ":filename: some_random_id.parquet\n",
    "file_path,pos\n",
    "s3://warehouse/house_prices/raw/data/00000-0-0ab09c23-d71c-4686-968a-f5ebd7b2e32a.parquet,0\n",
    "s3://warehouse/house_prices/raw/data/00000-0-0ab09c23-d71c-4686-968a-f5ebd7b2e32a.parquet,1\n",
    "s3://warehouse/house_prices/raw/data/00000-0-0ab09c23-d71c-4686-968a-f5ebd7b2e32a.parquet,2\n",
    "s3://warehouse/house_prices/raw/data/00000-0-0ab09c23-d71c-4686-968a-f5ebd7b2e32a.parquet,3\n",
    "```\n",
    "\n",
    "```{warning} Deprecation Warning\n",
    "Positional deletes will be replaced by deletion vectors in Iceberg V3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea148b7-02a9-482d-806c-bccd3e5ee580",
   "metadata": {},
   "source": [
    "Let's use pyiceberg to find a delete file and open it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dde4ab-043e-482e-b3b5-8dbdfaed993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_file = (\n",
    "    pl.from_arrow(house_prices_t.inspect.delete_files())\n",
    "    .select(pl.col(\"file_path\"))\n",
    "    .item(0, 0)\n",
    ")\n",
    "with pl.Config(fmt_str_lengths=100):\n",
    "    display(pl.read_parquet(fs.read_bytes(delete_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116c42a-ddaa-4800-a8b3-8fb69f3d4711",
   "metadata": {},
   "source": [
    "# Renaming and moving columns\n",
    "\n",
    "Iceberg implements all column references use the `field_id`. This makes it trivial to rename a column, since we just have to update the metadata of the schema. Imagine our style guide is updated and now all metadata fields such as our `_loaded_at` should now be prefixed with `dwh` to make it clear who did the load. Now that we have some hands-on user feedback, we also want to move `transfer_date` to be the first column since we're often visually exploring date ranges.\n",
    "\n",
    "We can also show off transactions - everything we've done until now has actually been done inside a transaction. We can explicitly open a transaction to perform multiple operations inside a single transaction. This includes deleting and adding files, but for now we'll just make our changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b6bec-2d12-4b62-b9c8-207552e6d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with house_prices_t.transaction() as transaction:\n",
    "    with transaction.update_schema() as update:\n",
    "        update.rename_column(\"_loaded_at\", \"_dwh_loaded_at\")\n",
    "        update.move_first(\"date_of_transfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42656e93-c6e3-4fb4-936b-d1d3d5113d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scan_iceberg(house_prices_t).head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac713db-ef2f-4dee-a057-cf600d305e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(get_iceberg_metadata(fs, house_prices_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
